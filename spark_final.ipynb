{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Setting up the environment\n",
        "installing the necessary components in the google colab env"
      ],
      "metadata": {
        "id": "tjCz2EYfEXnW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mj9ZgJ-wEVtB",
        "outputId": "6daaadd4-9d1c-452f-84d3-757914cba4d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.5)\n",
            "Collecting findspark\n",
            "  Downloading findspark-2.0.1-py2.py3-none-any.whl.metadata (352 bytes)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n",
            "Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
            "Installing collected packages: findspark\n",
            "Successfully installed findspark-2.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark findspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()"
      ],
      "metadata": {
        "id": "Mveh9RlvFZoQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"IMDB\").getOrCreate()"
      ],
      "metadata": {
        "id": "sO3BWpdYFjZs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "writing some tests to see that the current spark intallation has all the functionalities that will be needed"
      ],
      "metadata": {
        "id": "v7MSk-6vJnRn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
        "\n",
        "# 1. Create a small test DataFrame\n",
        "data = [(1, \"Alice\", 1980), (2, \"Bob\", 1990)]\n",
        "schema = StructType([\n",
        "    StructField(\"id\", IntegerType(), True),\n",
        "    StructField(\"name\", StringType(), True),\n",
        "    StructField(\"birthYear\", IntegerType(), True)\n",
        "])\n",
        "df = spark.createDataFrame(data, schema)\n",
        "df.show()\n",
        "\n",
        "# 2. Write to Parquet and read back\n",
        "test_path = \"/content/test_parquet\"\n",
        "df.write.mode(\"overwrite\").parquet(test_path)\n",
        "df_parquet = spark.read.parquet(test_path)\n",
        "print(\"âœ… Parquet read/write successful\")\n",
        "df_parquet.show()\n",
        "\n",
        "# 3. Simulate reading a TSV (like IMDB datasets)\n",
        "tsv_path = \"/content/test_people.tsv\"\n",
        "with open(tsv_path, \"w\") as f:\n",
        "    f.write(\"nconst\\tprimaryName\\tbirthYear\\n\")\n",
        "    f.write(\"nm0000001\\tFred Astaire\\t1899\\n\")\n",
        "    f.write(\"nm0000002\\tLauren Bacall\\t1924\\n\")\n",
        "\n",
        "df_tsv = spark.read.option(\"header\", True).option(\"sep\", \"\\t\").csv(tsv_path)\n",
        "print(\"âœ… TSV file loaded with inferred schema:\")\n",
        "df_tsv.printSchema()\n",
        "df_tsv.show()\n",
        "\n",
        "print(\"âœ… All tests passed. Spark is ready for the IMDB project\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sQq2FonoJs53",
        "outputId": "5ef22280-0362-4366-ffe5-ecc865a21e2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+---------+\n",
            "| id| name|birthYear|\n",
            "+---+-----+---------+\n",
            "|  1|Alice|     1980|\n",
            "|  2|  Bob|     1990|\n",
            "+---+-----+---------+\n",
            "\n",
            "âœ… Parquet read/write successful\n",
            "+---+-----+---------+\n",
            "| id| name|birthYear|\n",
            "+---+-----+---------+\n",
            "|  1|Alice|     1980|\n",
            "|  2|  Bob|     1990|\n",
            "+---+-----+---------+\n",
            "\n",
            "âœ… TSV file loaded with inferred schema:\n",
            "root\n",
            " |-- nconst: string (nullable = true)\n",
            " |-- primaryName: string (nullable = true)\n",
            " |-- birthYear: string (nullable = true)\n",
            "\n",
            "+---------+-------------+---------+\n",
            "|   nconst|  primaryName|birthYear|\n",
            "+---------+-------------+---------+\n",
            "|nm0000001| Fred Astaire|     1899|\n",
            "|nm0000002|Lauren Bacall|     1924|\n",
            "+---------+-------------+---------+\n",
            "\n",
            "âœ… All tests passed. Spark is ready for the IMDB project\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The data\n",
        "the data has been uploaded to a google [drive folder](https://drive.google.com/drive/folders/1mTUgXJaaOItl44Y91fYVoxSX95L_dL5W?usp=sharing)\n"
      ],
      "metadata": {
        "id": "rXgzmuQoOi0X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#https://drive.google.com/drive/folders/1mTUgXJaaOItl44Y91fYVoxSX95L_dL5W?usp=sharing\n",
        "!pip install -q gdown\n",
        "\n",
        "!gdown --folder --id 1mTUgXJaaOItl44Y91fYVoxSX95L_dL5W"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3L6PD9VbOiFY",
        "outputId": "1bcab108-fb5e-4e76-8b41-5fe39ff26880"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Retrieving folder contents\n",
            "Processing file 1LQMYnwYI6ClspulTrTZvq1_WdQKRbT0v name.basics.tsv.gz\n",
            "Processing file 1cBsbI-GGyspZpK6K9IAmzNr2UMspuBzF title.akas.tsv.gz\n",
            "Processing file 1J3Tt5yprc7oN7QP6YPL9BlCWP4_CkVgS title.basics.tsv.gz\n",
            "Processing file 1zlsfX_2roQwS3H1DziTJsyxlDnmdCX3r title.crew.tsv.gz\n",
            "Processing file 1u3f71O-oSZM1sXic4pl4iZxZbH6S5rRk title.episode.tsv.gz\n",
            "Processing file 1zHMfFEs6IWuD2ZUSsx9U85zkZYJmCVfu title.principals.tsv.gz\n",
            "Processing file 1YqsHPfa6Kz2b1xOiJCgtQG1wS_DLJnPe title.ratings.tsv.gz\n",
            "Retrieving folder contents completed\n",
            "Building directory structure\n",
            "Building directory structure completed\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1LQMYnwYI6ClspulTrTZvq1_WdQKRbT0v\n",
            "From (redirected): https://drive.google.com/uc?id=1LQMYnwYI6ClspulTrTZvq1_WdQKRbT0v&confirm=t&uuid=7a9c2f29-eabb-4c01-a34a-1180121b8d50\n",
            "To: /content/imdb-data/name.basics.tsv.gz\n",
            "100% 283M/283M [00:01<00:00, 166MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1cBsbI-GGyspZpK6K9IAmzNr2UMspuBzF\n",
            "From (redirected): https://drive.google.com/uc?id=1cBsbI-GGyspZpK6K9IAmzNr2UMspuBzF&confirm=t&uuid=1232cafc-4057-44fd-8fc1-87156b1b543f\n",
            "To: /content/imdb-data/title.akas.tsv.gz\n",
            "100% 450M/450M [00:02<00:00, 170MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1J3Tt5yprc7oN7QP6YPL9BlCWP4_CkVgS\n",
            "From (redirected): https://drive.google.com/uc?id=1J3Tt5yprc7oN7QP6YPL9BlCWP4_CkVgS&confirm=t&uuid=2f1a6e7e-8f20-4500-ad16-b0ffa0091b33\n",
            "To: /content/imdb-data/title.basics.tsv.gz\n",
            "100% 204M/204M [00:02<00:00, 84.1MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1zlsfX_2roQwS3H1DziTJsyxlDnmdCX3r\n",
            "From (redirected): https://drive.google.com/uc?id=1zlsfX_2roQwS3H1DziTJsyxlDnmdCX3r&confirm=t&uuid=e876ef24-1cdc-4050-affa-3687a8b1a9a0\n",
            "To: /content/imdb-data/title.crew.tsv.gz\n",
            "100% 75.7M/75.7M [00:00<00:00, 181MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1u3f71O-oSZM1sXic4pl4iZxZbH6S5rRk\n",
            "From (redirected): https://drive.google.com/uc?id=1u3f71O-oSZM1sXic4pl4iZxZbH6S5rRk&confirm=t&uuid=8ef71ac4-29c4-47be-9f64-5bba99fa13ae\n",
            "To: /content/imdb-data/title.episode.tsv.gz\n",
            "100% 49.1M/49.1M [00:00<00:00, 215MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1zHMfFEs6IWuD2ZUSsx9U85zkZYJmCVfu\n",
            "From (redirected): https://drive.google.com/uc?id=1zHMfFEs6IWuD2ZUSsx9U85zkZYJmCVfu&confirm=t&uuid=b7980bed-f310-4dc8-8de5-0b64b2e15705\n",
            "To: /content/imdb-data/title.principals.tsv.gz\n",
            "100% 711M/711M [00:08<00:00, 84.9MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1YqsHPfa6Kz2b1xOiJCgtQG1wS_DLJnPe\n",
            "To: /content/imdb-data/title.ratings.tsv.gz\n",
            "100% 7.83M/7.83M [00:00<00:00, 141MB/s]\n",
            "Download completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(os.listdir(\"/content/imdb-data\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JaI-_tc6PjQb",
        "outputId": "c353e9f7-c544-4b91-d3e1-442c93e15eb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['name.basics.tsv.gz', 'title.akas.tsv.gz', 'title.ratings.tsv.gz', 'title.crew.tsv.gz', 'title.basics.tsv.gz', 'title.episode.tsv.gz', 'title.principals.tsv.gz']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inspecting the data and structure"
      ],
      "metadata": {
        "id": "4kaY-hCyWHpu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import DataFrame\n",
        "\n",
        "# Define path and file list\n",
        "imdb_path = \"/content/imdb-data\"\n",
        "files = {\n",
        "    \"name.basics\": \"name.basics.tsv.gz\",\n",
        "    \"title.basics\": \"title.basics.tsv.gz\",\n",
        "    \"title.ratings\": \"title.ratings.tsv.gz\",\n",
        "    \"title.akas\": \"title.akas.tsv.gz\",\n",
        "    \"title.principals\": \"title.principals.tsv.gz\",\n",
        "    \"title.crew\": \"title.crew.tsv.gz\",\n",
        "    \"title.episode\": \"title.episode.tsv.gz\"\n",
        "}\n",
        "\n",
        "# Function to inspect structure\n",
        "def inspect_file(name: str, filename: str):\n",
        "    print(f\"\\nðŸ“ folder: {name} â€” {filename}\")\n",
        "    df = spark.read.option(\"header\", True).option(\"sep\", \"\\t\").csv(os.path.join(imdb_path, filename))\n",
        "    print(\"â–¶ Columns:\", df.columns)\n",
        "    print(\"â–¶ Schema:\")\n",
        "    df.printSchema()\n",
        "    print(\"â–¶ Row count (approx):\", df.count())\n",
        "    print(\"â–¶ Sample rows:\")\n",
        "    df.show(5, truncate=False)\n",
        "\n",
        "# Loop through and inspect all files\n",
        "for name, filename in files.items():\n",
        "    inspect_file(name, filename)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z-xW0XPJV0Rv",
        "outputId": "869fe6c0-1751-41e0-f9b6-2e1c44f7d67e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“ folder: name.basics â€” name.basics.tsv.gz\n",
            "â–¶ Columns: ['nconst', 'primaryName', 'birthYear', 'deathYear', 'primaryProfession', 'knownForTitles']\n",
            "â–¶ Schema:\n",
            "root\n",
            " |-- nconst: string (nullable = true)\n",
            " |-- primaryName: string (nullable = true)\n",
            " |-- birthYear: string (nullable = true)\n",
            " |-- deathYear: string (nullable = true)\n",
            " |-- primaryProfession: string (nullable = true)\n",
            " |-- knownForTitles: string (nullable = true)\n",
            "\n",
            "â–¶ Row count (approx): 14323600\n",
            "â–¶ Sample rows:\n",
            "+---------+---------------+---------+---------+----------------------------------+---------------------------------------+\n",
            "|nconst   |primaryName    |birthYear|deathYear|primaryProfession                 |knownForTitles                         |\n",
            "+---------+---------------+---------+---------+----------------------------------+---------------------------------------+\n",
            "|nm0000001|Fred Astaire   |1899     |1987     |actor,miscellaneous,producer      |tt0072308,tt0050419,tt0027125,tt0031983|\n",
            "|nm0000002|Lauren Bacall  |1924     |2014     |actress,soundtrack,archive_footage|tt0037382,tt0075213,tt0117057,tt0038355|\n",
            "|nm0000003|Brigitte Bardot|1934     |\\N       |actress,music_department,producer |tt0057345,tt0049189,tt0056404,tt0054452|\n",
            "|nm0000004|John Belushi   |1949     |1982     |actor,writer,music_department     |tt0072562,tt0077975,tt0080455,tt0078723|\n",
            "|nm0000005|Ingmar Bergman |1918     |2007     |writer,director,actor             |tt0050986,tt0069467,tt0050976,tt0083922|\n",
            "+---------+---------------+---------+---------+----------------------------------+---------------------------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "\n",
            "ðŸ“ folder: title.basics â€” title.basics.tsv.gz\n",
            "â–¶ Columns: ['tconst', 'titleType', 'primaryTitle', 'originalTitle', 'isAdult', 'startYear', 'endYear', 'runtimeMinutes', 'genres']\n",
            "â–¶ Schema:\n",
            "root\n",
            " |-- tconst: string (nullable = true)\n",
            " |-- titleType: string (nullable = true)\n",
            " |-- primaryTitle: string (nullable = true)\n",
            " |-- originalTitle: string (nullable = true)\n",
            " |-- isAdult: string (nullable = true)\n",
            " |-- startYear: string (nullable = true)\n",
            " |-- endYear: string (nullable = true)\n",
            " |-- runtimeMinutes: string (nullable = true)\n",
            " |-- genres: string (nullable = true)\n",
            "\n",
            "â–¶ Row count (approx): 11578517\n",
            "â–¶ Sample rows:\n",
            "+---------+---------+----------------------+----------------------+-------+---------+-------+--------------+------------------------+\n",
            "|tconst   |titleType|primaryTitle          |originalTitle         |isAdult|startYear|endYear|runtimeMinutes|genres                  |\n",
            "+---------+---------+----------------------+----------------------+-------+---------+-------+--------------+------------------------+\n",
            "|tt0000001|short    |Carmencita            |Carmencita            |0      |1894     |\\N     |1             |Documentary,Short       |\n",
            "|tt0000002|short    |Le clown et ses chiens|Le clown et ses chiens|0      |1892     |\\N     |5             |Animation,Short         |\n",
            "|tt0000003|short    |Poor Pierrot          |Pauvre Pierrot        |0      |1892     |\\N     |5             |Animation,Comedy,Romance|\n",
            "|tt0000004|short    |Un bon bock           |Un bon bock           |0      |1892     |\\N     |12            |Animation,Short         |\n",
            "|tt0000005|short    |Blacksmith Scene      |Blacksmith Scene      |0      |1893     |\\N     |1             |Short                   |\n",
            "+---------+---------+----------------------+----------------------+-------+---------+-------+--------------+------------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "\n",
            "ðŸ“ folder: title.ratings â€” title.ratings.tsv.gz\n",
            "â–¶ Columns: ['tconst', 'averageRating', 'numVotes']\n",
            "â–¶ Schema:\n",
            "root\n",
            " |-- tconst: string (nullable = true)\n",
            " |-- averageRating: string (nullable = true)\n",
            " |-- numVotes: string (nullable = true)\n",
            "\n",
            "â–¶ Row count (approx): 1556541\n",
            "â–¶ Sample rows:\n",
            "+---------+-------------+--------+\n",
            "|tconst   |averageRating|numVotes|\n",
            "+---------+-------------+--------+\n",
            "|tt0000001|5.7          |2148    |\n",
            "|tt0000002|5.5          |292     |\n",
            "|tt0000003|6.5          |2182    |\n",
            "|tt0000004|5.3          |188     |\n",
            "|tt0000005|6.2          |2925    |\n",
            "+---------+-------------+--------+\n",
            "only showing top 5 rows\n",
            "\n",
            "\n",
            "ðŸ“ folder: title.akas â€” title.akas.tsv.gz\n",
            "â–¶ Columns: ['titleId', 'ordering', 'title', 'region', 'language', 'types', 'attributes', 'isOriginalTitle']\n",
            "â–¶ Schema:\n",
            "root\n",
            " |-- titleId: string (nullable = true)\n",
            " |-- ordering: string (nullable = true)\n",
            " |-- title: string (nullable = true)\n",
            " |-- region: string (nullable = true)\n",
            " |-- language: string (nullable = true)\n",
            " |-- types: string (nullable = true)\n",
            " |-- attributes: string (nullable = true)\n",
            " |-- isOriginalTitle: string (nullable = true)\n",
            "\n",
            "â–¶ Row count (approx): 51863206\n",
            "â–¶ Sample rows:\n",
            "+---------+--------+-------------------------+------+--------+-----------+-------------+---------------+\n",
            "|titleId  |ordering|title                    |region|language|types      |attributes   |isOriginalTitle|\n",
            "+---------+--------+-------------------------+------+--------+-----------+-------------+---------------+\n",
            "|tt0000001|1       |Carmencita               |\\N    |\\N      |original   |\\N           |1              |\n",
            "|tt0000001|2       |Carmencita               |DE    |\\N      |\\N         |literal title|0              |\n",
            "|tt0000001|3       |Carmencita               |US    |\\N      |imdbDisplay|\\N           |0              |\n",
            "|tt0000001|4       |Carmencita - spanyol tÃ¡nc|HU    |\\N      |imdbDisplay|\\N           |0              |\n",
            "|tt0000001|5       |ÎšÎ±ÏÎ¼ÎµÎ½ÏƒÎ¯Ï„Î±               |GR    |\\N      |imdbDisplay|\\N           |0              |\n",
            "+---------+--------+-------------------------+------+--------+-----------+-------------+---------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "\n",
            "ðŸ“ folder: title.principals â€” title.principals.tsv.gz\n",
            "â–¶ Columns: ['tconst', 'ordering', 'nconst', 'category', 'job', 'characters']\n",
            "â–¶ Schema:\n",
            "root\n",
            " |-- tconst: string (nullable = true)\n",
            " |-- ordering: string (nullable = true)\n",
            " |-- nconst: string (nullable = true)\n",
            " |-- category: string (nullable = true)\n",
            " |-- job: string (nullable = true)\n",
            " |-- characters: string (nullable = true)\n",
            "\n",
            "â–¶ Row count (approx): 91938655\n",
            "â–¶ Sample rows:\n",
            "+---------+--------+---------+---------------+-----------------------+----------+\n",
            "|tconst   |ordering|nconst   |category       |job                    |characters|\n",
            "+---------+--------+---------+---------------+-----------------------+----------+\n",
            "|tt0000001|1       |nm1588970|self           |\\N                     |[\"Self\"]  |\n",
            "|tt0000001|2       |nm0005690|director       |\\N                     |\\N        |\n",
            "|tt0000001|3       |nm0005690|producer       |producer               |\\N        |\n",
            "|tt0000001|4       |nm0374658|cinematographer|director of photography|\\N        |\n",
            "|tt0000002|1       |nm0721526|director       |\\N                     |\\N        |\n",
            "+---------+--------+---------+---------------+-----------------------+----------+\n",
            "only showing top 5 rows\n",
            "\n",
            "\n",
            "ðŸ“ folder: title.crew â€” title.crew.tsv.gz\n",
            "â–¶ Columns: ['tconst', 'directors', 'writers']\n",
            "â–¶ Schema:\n",
            "root\n",
            " |-- tconst: string (nullable = true)\n",
            " |-- directors: string (nullable = true)\n",
            " |-- writers: string (nullable = true)\n",
            "\n",
            "â–¶ Row count (approx): 11580516\n",
            "â–¶ Sample rows:\n",
            "+---------+---------+---------+\n",
            "|tconst   |directors|writers  |\n",
            "+---------+---------+---------+\n",
            "|tt0000001|nm0005690|\\N       |\n",
            "|tt0000002|nm0721526|\\N       |\n",
            "|tt0000003|nm0721526|nm0721526|\n",
            "|tt0000004|nm0721526|\\N       |\n",
            "|tt0000005|nm0005690|\\N       |\n",
            "+---------+---------+---------+\n",
            "only showing top 5 rows\n",
            "\n",
            "\n",
            "ðŸ“ folder: title.episode â€” title.episode.tsv.gz\n",
            "â–¶ Columns: ['tconst', 'parentTconst', 'seasonNumber', 'episodeNumber']\n",
            "â–¶ Schema:\n",
            "root\n",
            " |-- tconst: string (nullable = true)\n",
            " |-- parentTconst: string (nullable = true)\n",
            " |-- seasonNumber: string (nullable = true)\n",
            " |-- episodeNumber: string (nullable = true)\n",
            "\n",
            "â–¶ Row count (approx): 8909066\n",
            "â–¶ Sample rows:\n",
            "+---------+------------+------------+-------------+\n",
            "|tconst   |parentTconst|seasonNumber|episodeNumber|\n",
            "+---------+------------+------------+-------------+\n",
            "|tt0031458|tt32857063  |\\N          |\\N           |\n",
            "|tt0041951|tt0041038   |1           |9            |\n",
            "|tt0042816|tt0989125   |1           |17           |\n",
            "|tt0042889|tt0989125   |\\N          |\\N           |\n",
            "|tt0043426|tt0040051   |3           |42           |\n",
            "+---------+------------+------------+-------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "This folder contains the core `.tsv.gz` files provided by IMDb at [https://datasets.imdbws.com](https://datasets.imdbws.com), which are used throughout this project.\n",
        "\n",
        "### Dataset Files & Their Structure\n",
        "\n",
        "| File | Description | Key Columns |\n",
        "|------|-------------|-------------|\n",
        "| **name.basics.tsv.gz** | People info (actors, directors, writers, etc.) | `nconst`, `primaryName`, `birthYear`, `primaryProfession`, `knownForTitles` |\n",
        "| **title.basics.tsv.gz** | Title info (movies, shows, etc.) | `tconst`, `titleType`, `primaryTitle`, `startYear`, `runtimeMinutes`, `genres` |\n",
        "| **title.ratings.tsv.gz** | Ratings and votes | `tconst`, `averageRating`, `numVotes` |\n",
        "| **title.akas.tsv.gz** | Alternate titles across regions/languages | `titleId`, `title`, `region`, `types`, `isOriginalTitle` |\n",
        "| **title.principals.tsv.gz** | People involved in each title + their role | `tconst`, `nconst`, `category`, `job`, `characters` |\n",
        "| **title.crew.tsv.gz** | Directors and writers per title | `tconst`, `directors`, `writers` |\n",
        "| **title.episode.tsv.gz** | Episode-specific info for TV shows | `tconst`, `parentTconst`, `seasonNumber`, `episodeNumber` |\n",
        "\n",
        "### Notes on Format\n",
        "\n",
        "- All files are tab-separated (`.tsv`) and compressed with Gzip (`.gz`)\n",
        "- Null values are represented as `\\N`\n",
        "- All fields are loaded as strings initially and should be cast to appropriate types when needed (e.g., integers for `birthYear`, floats for `averageRating`)\n"
      ],
      "metadata": {
        "id": "jx1f5cipX3TY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 2-13"
      ],
      "metadata": {
        "id": "wpGFsvNHbYkv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2**. How many total people in data set\n",
        "\n",
        "in this code we can see that we have used `distinct()`.\n",
        "\n",
        "In theory, `nconst` should be unique per row â€” but using `distinct()` ensures we account for any accidental duplicates.\n",
        "\n"
      ],
      "metadata": {
        "id": "RN26LleHcOJn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**using a Spark DataFrame to compute the answer**"
      ],
      "metadata": {
        "id": "_bIO0fT5dn6B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the name.basics data\n",
        "people_df = spark.read.option(\"header\", True).option(\"sep\", \"\\t\").csv(\"/content/imdb-data/name.basics.tsv.gz\")\n",
        "# - 'option(\"header\", True)' tells Spark to treat the first line of the file as column names.\n",
        "# - 'option(\"sep\", \"\\t\")' specifies that the file is tab-separated (TSV format).\n",
        "# - '.csv(...)' is used even for TSV files because Spark treats it as a general delimited format.\n",
        "\n",
        "# Count unique people using nconst\n",
        "unique_people_count = people_df.select(\"nconst\").distinct().count()\n",
        "# - 'select(\"nconst\")' extracts just the person ID column.\n",
        "# - 'distinct()' ensures that we only count unique IDs (in case of duplicates).\n",
        "# - 'count()' returns the total number of distinct people.\n",
        "\n",
        "print(f\" Total unique people in the dataset: {unique_people_count}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H45fwu3VbnL7",
        "outputId": "e6282517-2a78-4373-951e-c52b3f62d88f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Total unique people in the dataset: 14323600\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "total people in the data set: **14323600**"
      ],
      "metadata": {
        "id": "6VALRtaGcrWa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.4.5.** Earliest birth year\n",
        "\n",
        "this question will be answered using **SQL commands**"
      ],
      "metadata": {
        "id": "sINuPbYUc6Ug"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#this is using dataframes just as a reference\n",
        "\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Filter out null birth years and cast to int\n",
        "valid_births_df = people_df.filter(people_df.birthYear != \"\\\\N\") \\\n",
        "                           .withColumn(\"birthYear\", col(\"birthYear\").cast(\"int\"))\n",
        "\n",
        "# Find the earliest birth year\n",
        "earliest_birth_year = valid_births_df.agg({\"birthYear\": \"min\"}).collect()[0][0]\n",
        "\n",
        "print(f\" Earliest birth year in the dataset: {earliest_birth_year}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VivkXSOHeebo",
        "outputId": "ccad4249-7236-4446-f541-e15c8cbe77c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Earliest birth year in the dataset: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To answer this question, we used **Spark SQL** by first creating a temporary view of the people dataset. We then ran a SQL query to:\n",
        "\n",
        "- Filter out invalid birth years (represented as `\\N`)\n",
        "- Cast the `birthYear` column from string to integer\n",
        "- Select the **minimum birth year** using the `MIN()` aggregation function\n",
        "\n",
        "This demonstrates how SQL queries can be executed on Spark DataFrames using the `spark.sql()` interface.\n"
      ],
      "metadata": {
        "id": "YSIo7M8Ke1Ht"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create or replace a temporary SQL view for the people dataset (it is just creating here, but kept in case of rerun and overall consistency)\n",
        "people_df.createOrReplaceTempView(\"people\")\n",
        "\n",
        "# Use Spark SQL to get the earliest birth year, filtering out nulls (\\N)\n",
        "earliest_birth_year_sql = spark.sql(\"\"\"\n",
        "    SELECT MIN(CAST(birthYear AS INT)) AS earliest_birth_year\n",
        "    FROM people\n",
        "    WHERE birthYear != '\\\\N'\n",
        "\"\"\")\n",
        "\n",
        "# Show the result\n",
        "earliest_birth_year_sql.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9VV3rdYPdODN",
        "outputId": "a9baad24-9507-4b7e-e920-7eed57fcaf40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+\n",
            "|earliest_birth_year|\n",
            "+-------------------+\n",
            "|                  4|\n",
            "+-------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "while this seems like a suspect result since the person is born more than **2020~2021 years ago**.\n",
        "we'll check out more details before coming to a conclusion"
      ],
      "metadata": {
        "id": "Li94ZKage_uR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"\"\"\n",
        "    SELECT nconst, primaryName, birthYear\n",
        "    FROM people\n",
        "    WHERE birthYear = '4'\n",
        "\"\"\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-u15sBnpfFPu",
        "outputId": "8f18e693-7e8f-41b2-9f1d-434600522c2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+------------------+---------+\n",
            "|   nconst|       primaryName|birthYear|\n",
            "+---------+------------------+---------+\n",
            "|nm0784172|Lucio Anneo Seneca|        4|\n",
            "+---------+------------------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "explaining the result:\n",
        "\n",
        "Using Spark SQL, we found that the **earliest year of birth** recorded in the dataset is **4 AD**.\n",
        "\n",
        "This result may seem surprising at first, but it makes sense given the nature of IMDb's data. IMDb includes not only modern actors and filmmakers but also historical figures who are referenced in documentaries, educational series, or dramatized content.\n",
        "\n",
        "For example in this case, the person with `birthYear = 4` is:\n",
        "\n",
        "- **Lucio Anneo Seneca**, a Roman philosopher, listed in the dataset with `nconst = nm0784172`.\n",
        "\n",
        "These historical entries are valid, but depending on the scope of the analysis, we might choose to **filter out extremely early birth years** (e.g., before the year 1000) for more modern-focused insights.\n",
        "\n",
        "we don't know if we should filter them out so far, so they will be kept unless we encouter an issue\n"
      ],
      "metadata": {
        "id": "Z-a7nN4eflDh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6.** latest year of birth\n"
      ],
      "metadata": {
        "id": "bdm9W1f3grbE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Latest year of birth using Spark SQL\n",
        "latest_birth_year_sql = spark.sql(\"\"\"\n",
        "    SELECT MAX(CAST(birthYear AS INT)) AS latest_birth_year\n",
        "    FROM people\n",
        "    WHERE birthYear != '\\\\N'\n",
        "\"\"\")\n",
        "\n",
        "latest_birth_year_sql.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cX9-HAo0gtsJ",
        "outputId": "1fd957aa-5fe8-4c4c-dd77-8aafe87c2bb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+\n",
            "|latest_birth_year|\n",
            "+-----------------+\n",
            "|             2024|\n",
            "+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We used a **Spark SQL query** to determine the most recent birth year in the dataset. The steps included:\n",
        "\n",
        "- Filtering out null values (`\\N`) in the `birthYear` column\n",
        "- Casting the `birthYear` values from string to integer\n",
        "- Using the SQL `MAX()` function to find the most recent year\n",
        "\n",
        "This gives us insight into how up-to-date the dataset is in terms of the people it includes. The latest birth year often corresponds to very young individuals who have recently appeared in films or television.\n"
      ],
      "metadata": {
        "id": "7Krz0p6fgxYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7.** People with missing birth year"
      ],
      "metadata": {
        "id": "3fl7_Q-zhNFO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Count how many people have no recorded birth year\n",
        "missing_birth_count = people_df.filter(people_df.birthYear == \"\\\\N\").count()\n",
        "# - IMDb represents missing values as the string '\\N', not as null.\n",
        "# - 'filter(...)' selects rows where the birthYear equals '\\N'.\n",
        "# - 'count()' returns the total number of such rows.\n",
        "print(f\"Number of people with no birth year: {missing_birth_count}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LjvYeK1ShQfP",
        "outputId": "d35c6c54-18ac-49b3-8e88-f60703a5182c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of people with no birth year: 13680662\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "people with no birth year in the dataset: **13680662**"
      ],
      "metadata": {
        "id": "fbJ0NLOyhoE-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**8.9.** Length of longest short and shortest movie after 1900\n",
        "\n",
        "We'll use `title.basics.tsv.gz`, which includes:\n",
        "\n",
        "- `titleType` (to filter for \"short\" or \"movie\")\n",
        "\n",
        "- `startYear` (to filter for titles after 1900)\n",
        "\n",
        "- `runtimeMinutes` (to find the min/max length)\n",
        "\n"
      ],
      "metadata": {
        "id": "snxAMpUXh7vh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load title.basics data\n",
        "titles_df = spark.read.option(\"header\", True).option(\"sep\", \"\\t\").csv(\"/content/imdb-data/title.basics.tsv.gz\")"
      ],
      "metadata": {
        "id": "eT7dzgRXiUn6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter out invalid/null runtime and startYear values, and cast them to integers\n",
        "# we could also add a condition on the year here, but this cell is kept for null values only and a condition on the year will be with the length on the next cells\n",
        "clean_titles_df = titles_df.filter((col(\"startYear\") != \"\\\\N\") & (col(\"runtimeMinutes\") != \"\\\\N\")) \\\n",
        "    .withColumn(\"startYear\", col(\"startYear\").cast(\"int\")) \\\n",
        "    .withColumn(\"runtimeMinutes\", col(\"runtimeMinutes\").cast(\"int\"))"
      ],
      "metadata": {
        "id": "DZLvrxHqiaMc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# longest \"short\" after 1900\n",
        "\n",
        "# - Filter where titleType is \"short\" and startYear > 1900\n",
        "# - Use max() to get the longest runtime\n",
        "longest_short_runtime = clean_titles_df \\\n",
        "    .filter((col(\"titleType\") == \"short\") & (col(\"startYear\") > 1900)) \\\n",
        "    .agg({\"runtimeMinutes\": \"max\"}) \\\n",
        "    .collect()[0][0]\n",
        "\n",
        "# this step is optional but can help make things more readable or in a more standard format\n",
        "# convert runtime to hours and minutes\n",
        "hours = longest_short_runtime // 60\n",
        "minutes = longest_short_runtime % 60\n",
        "formatted_runtime = f\"{hours}h {minutes}m\" if hours > 0 else f\"{minutes}m\"\n",
        "\n",
        "print(f\"Longest short after 1900: {longest_short_runtime} minutes or {formatted_runtime}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PVoXSNLwi1O9",
        "outputId": "1cad6e87-8da2-452d-ee80-5e572155630a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Longest short after 1900: 250 minutes or 4h 10m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Longest short after 1900: 250 minutes or 4h 10m"
      ],
      "metadata": {
        "id": "bgHUONOAjynf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# shortest \"movie\" after 1900\n",
        "\n",
        "# - Filter where titleType is \"movie\" and startYear > 1900\n",
        "# - Use min() to get the shortest runtime\n",
        "shortest_movie_runtime = clean_titles_df \\\n",
        "    .filter((col(\"titleType\") == \"movie\") & (col(\"startYear\") > 1900)) \\\n",
        "    .agg({\"runtimeMinutes\": \"min\"}) \\\n",
        "    .collect()[0][0]\n",
        "\n",
        "# convert runtime to hours and minutes\n",
        "hours = shortest_movie_runtime // 60\n",
        "minutes = shortest_movie_runtime % 60\n",
        "formatted_runtime = f\"{hours}h {minutes}m\" if hours > 0 else f\"{minutes}m\"\n",
        "\n",
        "print(f\"Shortest movie after 1900: {shortest_movie_runtime} minutes\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZ1NZNrJjefx",
        "outputId": "6d17ada6-11e5-44ab-8b70-652b1a3eb5ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shortest movie after 1900: 1 minutes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Shortest movie after 1900: 1 minutes"
      ],
      "metadata": {
        "id": "4J7ylO4gj2XI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "while this seems very short, it's not a mistake and we can look up movies with a minute length to check that it is indeed not a mistake"
      ],
      "metadata": {
        "id": "LEpkUvQfkLcN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Find titles of 1-minute movies released after 1900\n",
        "shortest_movie_df = clean_titles_df \\\n",
        "    .filter((col(\"titleType\") == \"movie\") &\n",
        "            (col(\"startYear\") > 1900) &\n",
        "            (col(\"runtimeMinutes\") == 1)) \\\n",
        "    .select(\"tconst\", \"primaryTitle\", \"startYear\", \"runtimeMinutes\")\n",
        "\n",
        "shortest_movie_df.show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K-FFKp5ekB3N",
        "outputId": "254b5c18-bc87-4161-924a-ac9f3648c40d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-------------------------+---------+--------------+\n",
            "|tconst    |primaryTitle             |startYear|runtimeMinutes|\n",
            "+----------+-------------------------+---------+--------------+\n",
            "|tt0025166 |George White's Scandals  |1934     |1             |\n",
            "|tt0469119 |Love Trap                |2005     |1             |\n",
            "|tt0810779 |Bound by Blood           |2007     |1             |\n",
            "|tt0848384 |Nikkatsu on Parade       |1930     |1             |\n",
            "|tt12893768|If I Die Tomorrow        |2020     |1             |\n",
            "|tt26348770|Dancing Boy              |2023     |1             |\n",
            "|tt32276067|Honest Vikky (Life Coach)|2024     |1             |\n",
            "+----------+-------------------------+---------+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**10.** list of all of the genres represented\n"
      ],
      "metadata": {
        "id": "4fUBbJLVkfc3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import explode, split\n",
        "\n",
        "# Step 1: Filter out rows with null genres\n",
        "non_null_genres_df = titles_df.filter(titles_df.genres != \"\\\\N\")\n",
        "\n",
        "# Step 2: Split the comma-separated genres into arrays\n",
        "split_genres_df = non_null_genres_df.withColumn(\"genre\", explode(split(\"genres\", \",\")))\n",
        "\n",
        "# Step 3: Select distinct genres\n",
        "unique_genres_df = split_genres_df.select(\"genre\").distinct()\n",
        "\n",
        "# Show all unique genres\n",
        "unique_genres_df.show(100 , truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PY4Eqow3krKm",
        "outputId": "9486cacb-4ec7-4950-dfea-f113a8b0fab8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+\n",
            "|genre      |\n",
            "+-----------+\n",
            "|Crime      |\n",
            "|Romance    |\n",
            "|Thriller   |\n",
            "|Adventure  |\n",
            "|Drama      |\n",
            "|War        |\n",
            "|Documentary|\n",
            "|Reality-TV |\n",
            "|Family     |\n",
            "|Fantasy    |\n",
            "|Game-Show  |\n",
            "|Adult      |\n",
            "|History    |\n",
            "|Mystery    |\n",
            "|Musical    |\n",
            "|Animation  |\n",
            "|Music      |\n",
            "|Film-Noir  |\n",
            "|Short      |\n",
            "|Horror     |\n",
            "|Western    |\n",
            "|Biography  |\n",
            "|Comedy     |\n",
            "|Sport      |\n",
            "|Action     |\n",
            "|Talk-Show  |\n",
            "|Sci-Fi     |\n",
            "|News       |\n",
            "+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "by default, the number of rows displayed by `show()` is limited to 20, we set the limit to 100 manually which is enough because it was set retroactively after the following cells from which we know the length is actually 28.\n",
        "\n",
        "But we e can do it another way if we don't know the length"
      ],
      "metadata": {
        "id": "AMEMNen3lgQb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "genres = [row[\"genre\"] for row in unique_genres_df.collect()]\n",
        "print(genres)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0YnqZLmwlfwD",
        "outputId": "cf992758-b215-41ad-a96c-00fb3bf726e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Crime', 'Romance', 'Thriller', 'Adventure', 'Drama', 'War', 'Documentary', 'Reality-TV', 'Family', 'Fantasy', 'Game-Show', 'Adult', 'History', 'Mystery', 'Musical', 'Animation', 'Music', 'Film-Noir', 'Short', 'Horror', 'Western', 'Biography', 'Comedy', 'Sport', 'Action', 'Talk-Show', 'Sci-Fi', 'News']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(genres)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84lgd-zZlyKw",
        "outputId": "6d1f3d70-e6f2-40c7-9d48-3f8d58dcfd53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "28"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To answer this question, we extracted the unique genres from the `title.basics.tsv.gz` file using the following steps:\n",
        "\n",
        "1. **Filtered out null genre entries** (`\\N`)\n",
        "2. **Split** the `genres` column, which contains comma-separated strings (e.g., `\"Action,Drama\"`), into arrays\n",
        "3. **Exploded** the arrays so that each genre appears in its own row\n",
        "4. Selected the **distinct** genre values\n",
        "\n",
        "This allowed us to get a clean list of all genres represented across titles in the dataset.\n"
      ],
      "metadata": {
        "id": "e4ijmznolFKn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**11.** highest rated comedy movie in the dataset"
      ],
      "metadata": {
        "id": "PTuZBVf3mYCX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load title basics and ratings\n",
        "ratings_df = spark.read.option(\"header\", True).option(\"sep\", \"\\t\").csv(\"/content/imdb-data/title.ratings.tsv.gz\")\n",
        "titles_df = spark.read.option(\"header\", True).option(\"sep\", \"\\t\").csv(\"/content/imdb-data/title.basics.tsv.gz\")\n",
        "\n",
        "# Filter titles: type = 'movie', genre includes 'Comedy', startYear is not null\n",
        "comedy_movies_df = titles_df \\\n",
        "    .filter((col(\"titleType\") == \"movie\") &\n",
        "            (col(\"genres\").contains(\"Comedy\")) &\n",
        "            (col(\"startYear\") != \"\\\\N\"))\n",
        "\n",
        "# Join with ratings on tconst\n",
        "comedy_rated_df = comedy_movies_df.join(ratings_df, on=\"tconst\")\n",
        "\n",
        "# Cast ratings and votes to numeric types\n",
        "comedy_rated_df = comedy_rated_df \\\n",
        "    .withColumn(\"averageRating\", col(\"averageRating\").cast(\"double\")) \\\n",
        "    .withColumn(\"numVotes\", col(\"numVotes\").cast(\"int\"))\n",
        "\n",
        "# Find the highest rated comedy movie, breaking ties by highest number of votes\n",
        "# this can be done directly with the following line, but we will make things more explicit on the output\n",
        "# top_comedy = comedy_rated_df.orderBy(col(\"averageRating\").desc(), col(\"numVotes\").desc()).limit(1)\n",
        "# top_comedy.select(\"primaryTitle\", \"averageRating\", \"numVotes\", \"tconst\").show(truncate=False)\n",
        "\n",
        "from pyspark.sql.functions import max as spark_max\n",
        "\n",
        "# the highest rating value for comedy movies\n",
        "max_rating = comedy_rated_df.agg(spark_max(\"averageRating\")).collect()[0][0]\n",
        "\n",
        "# all movies with that same rating\n",
        "tied_movies_df = comedy_rated_df.filter(col(\"averageRating\") == max_rating) \\\n",
        "    .orderBy(col(\"numVotes\").desc())\n",
        "\n",
        "# all tied top-rated comedy movies\n",
        "print(\" Movies tied with the highest comedy rating:\")\n",
        "tied_movies_df.select(\"primaryTitle\", \"averageRating\", \"numVotes\", \"tconst\").show(truncate=False)\n",
        "\n",
        "# the top one by vote count\n",
        "top_comedy_movie = tied_movies_df.limit(1)\n",
        "\n",
        "print(\" Final selected highest-rated comedy movie (tie broken by votes):\")\n",
        "top_comedy_movie.select(\"primaryTitle\", \"averageRating\", \"numVotes\", \"tconst\").show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DdzyM1-wm3kk",
        "outputId": "f2a20c3e-c67d-46be-ae32-ea7fcdb64402"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Movies tied with the highest comedy rating:\n",
            "+----------------------+-------------+--------+----------+\n",
            "|primaryTitle          |averageRating|numVotes|tconst    |\n",
            "+----------------------+-------------+--------+----------+\n",
            "|Here They Go          |10.0         |8       |tt20115996|\n",
            "|Planet Disagreements 8|10.0         |8       |tt21360086|\n",
            "|The Premiere          |10.0         |8       |tt26771891|\n",
            "|Meet the Radebes      |10.0         |8       |tt6999602 |\n",
            "+----------------------+-------------+--------+----------+\n",
            "\n",
            " Final selected highest-rated comedy movie (tie broken by votes):\n",
            "+------------+-------------+--------+----------+\n",
            "|primaryTitle|averageRating|numVotes|tconst    |\n",
            "+------------+-------------+--------+----------+\n",
            "|Here They Go|10.0         |8       |tt20115996|\n",
            "+------------+-------------+--------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To determine the highest rated comedy movie, we used two datasets:\n",
        "\n",
        "- `title.basics.tsv.gz` â€” contains title metadata (type, genres, name)\n",
        "- `title.ratings.tsv.gz` â€” provides average IMDb ratings and vote counts\n",
        "\n",
        "#### Steps:\n",
        "1. **Filtered** titles to include only:\n",
        "   - Movies (`titleType = \"movie\"`)\n",
        "   - With \"Comedy\" in the `genres` column\n",
        "   - That have a valid `startYear`\n",
        "2. **Joined** this filtered set with the ratings data on the `tconst` column.\n",
        "3. **Cast** `averageRating` and `numVotes` to appropriate numeric types.\n",
        "4. **Computed** the maximum `averageRating` among the filtered results.\n",
        "5. **Filtered again** to get all comedy movies with this top rating.\n",
        "6. **Displayed all tied movies**, sorted by number of votes.\n",
        "7. **Selected the final winner** by choosing the one with the highest number of votes.\n",
        "\n",
        "This approach ensures that if there's a tie in rating, we explicitly show all tied titles and then clearly identify the winning movie based on popularity (most votes)."
      ],
      "metadata": {
        "id": "3v5LubeDm33D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "while we did pick a movie, we can notice that they had ties at 10, and that the number of votes is also the same at 8.\n",
        "\n",
        "breaking the tie is actually impossible but since one had to be picked, the first one that came up on the table was."
      ],
      "metadata": {
        "id": "YGwXKp0lpCqD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**12.** director of the top rated movie"
      ],
      "metadata": {
        "id": "HGyfXH1RpcGP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the tconst of the top-rated comedy movie\n",
        "top_movie_tconst = top_comedy_movie.select(\"tconst\").collect()[0][0]\n",
        "# - 'top_comedy_movie' is a DataFrame containing the top movie's details.\n",
        "# - '.select(\"tconst\")' selects the 'tconst' column, which is the unique identifier for titles.\n",
        "# - '.collect()' collects the result into a list of Row objects.\n",
        "# - '[0]' accesses the first (and only) row in the result.\n",
        "# - '[0]' again accesses the 'tconst' value from the Row object (Row objects behave like tuples).\n",
        "\n",
        "# Load the title.crew dataset\n",
        "crew_df = spark.read.option(\"header\", True).option(\"sep\", \"\\t\").csv(\"/content/imdb-data/title.crew.tsv.gz\")\n",
        "\n",
        "# Filter for the movie's director(s)\n",
        "director_df = crew_df.filter(crew_df.tconst == top_movie_tconst).select(\"directors\")\n",
        "\n",
        "# Get the director's nconst from the previous step\n",
        "director_nconst = director_df.collect()[0][0]\n",
        "\n",
        "# Load the name.basics dataset to get the director's name\n",
        "people_df = spark.read.option(\"header\", True).option(\"sep\", \"\\t\").csv(\"/content/imdb-data/name.basics.tsv.gz\")\n",
        "\n",
        "# Filter the dataset to get the director's name using the nconst\n",
        "director_name_df = people_df.filter(people_df.nconst == director_nconst).select(\"primaryName\")\n",
        "\n",
        "# Show the director's name\n",
        "director_name_df.show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0LF5bT3ypbB3",
        "outputId": "c3bd88f1-6a71-4ef7-dd7c-6d5e8dcb8243"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+\n",
            "|primaryName |\n",
            "+------------+\n",
            "|Bryan Bostic|\n",
            "+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**13.** alternate titles for the movie\n",
        "\n",
        "We'll need to pull data from the `title.akas.tsv.gz` file, which contains alternate titles in different languages or regions.\n",
        "\n",
        "We already have the `tconst` for the top comedy movie from Question 11, so we'll filter for that and list the alternate titles.\n",
        "\n"
      ],
      "metadata": {
        "id": "9OnuP8fTqr9l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the title.akas dataset\n",
        "akas_df = spark.read.option(\"header\", True).option(\"sep\", \"\\t\").csv(\"/content/imdb-data/title.akas.tsv.gz\")\n",
        "\n",
        "# Filter for the top-rated comedy movie tconst\n",
        "alternate_titles_df = akas_df.filter(akas_df.titleId == top_movie_tconst).select(\"title\")\n",
        "\n",
        "# Show all alternate titles\n",
        "alternate_titles_df.show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K61D4-ghrJzz",
        "outputId": "537dc680-8703-4669-82ba-ea3848015590"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+\n",
            "|title       |\n",
            "+------------+\n",
            "|Here They Go|\n",
            "|Here They Go|\n",
            "+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Degrees of separation\n"
      ],
      "metadata": {
        "id": "RrKfRhj-HjN-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load name.basics dataset to find the name of nconst nm0000102\n",
        "people_df = spark.read.option(\"header\", True).option(\"sep\", \"\\t\").csv(\"/content/imdb-data/name.basics.tsv.gz\")\n",
        "\n",
        "# Filter to find the person with nconst = nm0000102\n",
        "person_info = people_df.filter(people_df.nconst == \"nm0000102\").select(\"primaryName\")\n",
        "\n",
        "# Show the result (the name of the person with nconst = nm0000102)\n",
        "person_info.show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WSpDchV2HlET",
        "outputId": "fa93af72-37f2-4688-c915-bb8e9e664c0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+\n",
            "|primaryName|\n",
            "+-----------+\n",
            "|Kevin Bacon|\n",
            "+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "this will be the *origin* of our degrees of separation.\n",
        "\n",
        "we can see it as a graph where each person is a node/vertex, and these are linked by edges which represent movies/shorts/shows.\n"
      ],
      "metadata": {
        "id": "v4vuOI0Kk0sq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Algorithm Overview: Calculating Degrees of Separation**\n",
        "\n",
        "To model the degrees of separation between people in the dataset, we use a **Breadth-First Search (BFS)** approach.\n",
        "\n",
        "1. **Starting Point**: We start from a specific **person** (referred to as the **origin**), represented by their **nconst** (e.g., `nm0000102`).\n",
        "\n",
        "2. **Degree 0**: The origin itself is considered to be at **Degree 0** (the first degree of separation). We initialize a **visited set** to track the people we've encountered and begin with the origin person.\n",
        "\n",
        "3. **Degree 1**: From the origin, we look at all **directly connected people** â€” these are the people who worked on the same movies as the origin (e.g., co-actors, directors, etc.). These people form **Degree 1**.\n",
        "\n",
        "4. **Track Visited Nodes**: As we explore each degree, we maintain a **set of visited nodes** to ensure that we don't count the same person more than once. If a person has already been encountered in a previous degree, we skip them.\n",
        "(If nodes A and B at degree `n` both link to node C which makes it link to degree `n+1`, we only need to keep one of them (for example, node A). This is because functionally, both nodes provide the same connection â€” the only point of reference that matters to us is the origin.)\n",
        "\n",
        "5. **Degree 2, Degree 3, and so on**: For each person in **Degree 1**, we find all **linked people** (people who worked on movies with them) to form **Degree 2**. We repeat this process for each subsequent degree, going as deep as necessary (up to **Degree 6**).\n",
        "\n",
        "6. **BFS Process**: At each degree, we:\n",
        "   - Find the people linked to those from the previous degree.\n",
        "   - **Filter out** people who have already been visited.\n",
        "   - Add these new people to the **visited set** and the current degree.\n",
        "   \n",
        "   This ensures that no one is counted multiple times and we explore the degrees layer by layer, starting from the origin and moving outward.\n",
        "\n",
        "7. **Termination**: The process continues until we've reached **Degree 6** (or whatever the maximum degree is if we want to be more general). Each degree's set of people is saved as a table for further analysis.\n",
        "\n",
        "---\n",
        "\n",
        "- **BFS Traversal**: This algorithm explores people level by level (degree by degree), ensuring all connections are explored systematically ([wiki](https://en.wikipedia.org/wiki/Breadth-first_search)).\n",
        "- **Visited Set**: Prevents revisiting nodes that have already been explored in previous degrees, maintaining the accuracy of the degree count.\n",
        "- **Linked Nodes**: These are the people connected through movies (shared `tconst`), forming edges in the graph of relationships.\n"
      ],
      "metadata": {
        "id": "o2E09qNNljZu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "from collections import deque\n",
        "\n",
        "# Define the starting person\n",
        "starting_person = 'nm0000102'\n",
        "\n",
        "# Create a set to track visited people (global to all degrees)\n",
        "visited_people = set()\n",
        "current_degree_people = set([starting_person])  # Start with the starting person"
      ],
      "metadata": {
        "id": "_Fbs-R3ckwxE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load title.principals dataset to get people linked to movies\n",
        "title_principals_df = spark.read.option(\"header\", True).option(\"sep\", \"\\t\").csv(\"/content/imdb-data/title.principals.tsv.gz\")\n"
      ],
      "metadata": {
        "id": "bsjLKLPoodAr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Output the size (number of rows) of the data\n",
        "size = title_principals_df.count()\n",
        "print(f\"Size of the dataset: {size} rows\")\n",
        "# this outputs Size of the dataset: 91938655 rows and takes a long time to count"
      ],
      "metadata": {
        "id": "B05e8wTz1v6y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aebe8359-32e7-40c2-ae53-7a6b82e9f536"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of the dataset: 91938655 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the column names of the DataFrame\n",
        "print(\"Column names in the DataFrame:\")\n",
        "print(title_principals_df.columns)\n",
        "\n",
        "# Optionally, display the first few rows of the DataFrame to inspect its structure\n",
        "title_principals_df.show(5, truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rFdn_QSUE4l_",
        "outputId": "da56afd5-e3fe-44b3-96f0-66e76f342b4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Column names in the DataFrame:\n",
            "['tconst', 'ordering', 'nconst', 'category', 'job', 'characters']\n",
            "+---------+--------+---------+---------------+-----------------------+----------+\n",
            "|tconst   |ordering|nconst   |category       |job                    |characters|\n",
            "+---------+--------+---------+---------------+-----------------------+----------+\n",
            "|tt0000001|1       |nm1588970|self           |\\N                     |[\"Self\"]  |\n",
            "|tt0000001|2       |nm0005690|director       |\\N                     |\\N        |\n",
            "|tt0000001|3       |nm0005690|producer       |producer               |\\N        |\n",
            "|tt0000001|4       |nm0374658|cinematographer|director of photography|\\N        |\n",
            "|tt0000002|1       |nm0721526|director       |\\N                     |\\N        |\n",
            "+---------+--------+---------+---------------+-----------------------+----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter for only actors and directors\n",
        "filtered_df = title_principals_df.filter(\n",
        "    title_principals_df.category.isin(\"actor\", \"director\")\n",
        ")\n",
        "\n",
        "# Show a preview of the filtered data\n",
        "filtered_df.show(5, truncate=False)\n",
        "\n",
        "# Output the size (number of rows) of the filtered dataset\n",
        "filtered_size = filtered_df.count()\n",
        "print(f\"Size of the filtered dataset (only actors and directors): {filtered_size} rows\")\n",
        "#this outputs: Size of the filtered dataset (only actors and directors): 29777295 rows, and it takes a long time\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "omNKGqt1GeqN",
        "outputId": "d3708e9c-66f4-4435-aef7-b1f54b39d498"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+--------+---------+--------+---+--------------+\n",
            "|tconst   |ordering|nconst   |category|job|characters    |\n",
            "+---------+--------+---------+--------+---+--------------+\n",
            "|tt0000001|2       |nm0005690|director|\\N |\\N            |\n",
            "|tt0000002|1       |nm0721526|director|\\N |\\N            |\n",
            "|tt0000003|1       |nm0721526|director|\\N |\\N            |\n",
            "|tt0000004|1       |nm0721526|director|\\N |\\N            |\n",
            "|tt0000005|1       |nm0443482|actor   |\\N |[\"Blacksmith\"]|\n",
            "+---------+--------+---------+--------+---+--------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "Size of the filtered dataset (only actors and directors): 29777295 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Randomly sample 10,000 rows from the filtered DataFrame\n",
        "sampled_df = filtered_df.sample(fraction=10000 / filtered_df.count(), seed=110)\n",
        "\n",
        "# Show the top 5 rows of the sampled data\n",
        "sampled_df.show(5, truncate=False)\n",
        "\n",
        "# Output the size (number of rows) of the sampled dataset\n",
        "sampled_size = sampled_df.count()\n",
        "print(f\"Size of the sampled dataset: {sampled_size} rows\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TNdNCjOAHnBY",
        "outputId": "63de64dd-d68b-4fc9-afac-6089f9678e7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+--------+---------+--------+---+--------------------------------------------+\n",
            "|tconst   |ordering|nconst   |category|job|characters                                  |\n",
            "+---------+--------+---------+--------+---+--------------------------------------------+\n",
            "|tt0001121|7       |nm0163559|actor   |\\N |[\"The Preacher\"]                            |\n",
            "|tt0002055|6       |nm0784407|director|\\N |\\N                                          |\n",
            "|tt0002282|5       |nm0649211|actor   |\\N |[\"Jack's Sweetheart's Father\"]              |\n",
            "|tt0002920|1       |nm0000779|actor   |\\N |[\"Fatty\"]                                   |\n",
            "|tt0003748|1       |nm0853336|actor   |\\N |[\"Robert W. Wainwright AKA Captain Alvarez\"]|\n",
            "+---------+--------+---------+--------+---+--------------------------------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "Size of the sampled dataset: 10068 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# A function to find all people connected to a given person in a particular degree (using title.principals)\n",
        "def find_connected_people(person_nconst):\n",
        "    print(f\"Processing person: {person_nconst}\")\n",
        "\n",
        "    # Get all the titles (tconst) the person worked on\n",
        "    titles = filtered_df.filter(filtered_df.nconst == person_nconst).select(\"tconst\")\n",
        "    # Get all people who worked on those titles (connected people)\n",
        "    connected_people = filtered_df.join(titles, \"tconst\").select(\"nconst\").distinct()\n",
        "\n",
        "    # Collect distinct connected people\n",
        "    return connected_people.rdd.map(lambda row: row['nconst']).collect()\n"
      ],
      "metadata": {
        "id": "JxYiXvEUn64k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#just to track progress\n",
        "!pip install tqdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PfNN0HD68C8x",
        "outputId": "250e1cc7-f71c-4035-b591-7b712473617d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For each degree (1 to 6)\n",
        "for degree in range(1, 7):\n",
        "    print(f\"Starting Degree {degree}...\")\n",
        "    new_degree_people = set()  # New set to track people for this degree\n",
        "    degree_count = 0  # Track the number of new people added in this degree\n",
        "    for person in current_degree_people:\n",
        "        # Get all people connected to the current person (who haven't been visited)\n",
        "        connected_people = find_connected_people(person)\n",
        "\n",
        "        for new_person in connected_people:\n",
        "            # Only add the person if they haven't been visited already\n",
        "            if new_person not in visited_people:\n",
        "                new_degree_people.add(new_person)\n",
        "                visited_people.add(new_person)  # Mark them as visited\n",
        "                degree_count += 1  # Count new person added\n",
        "\n",
        "    # If no new people were added to this degree, print a warning (indicating we might be stuck or at the limit)\n",
        "    if degree_count == 0:\n",
        "        print(f\"WARNING: No new people found for Degree {degree}. The traversal might be stuck or finished.\")\n",
        "        break\n",
        "\n",
        "    # Move to the next degree\n",
        "    current_degree_people = new_degree_people\n",
        "\n",
        "    # Create a DataFrame of the current degree people\n",
        "    degree_df = spark.createDataFrame([(p,) for p in current_degree_people], [\"nconst\"])\n",
        "\n",
        "    # Save the DataFrame to a Parquet file for this degree\n",
        "    degree_df.write.mode(\"overwrite\").parquet(f\"/content/degree_{degree}.parquet\")\n",
        "\n",
        "    # Print the saved degree for verification\n",
        "    print(f\"Degree {degree} saved to /content/degree_{degree}.parquet\")\n",
        "    print(f\"Number of new people in Degree {degree}: {degree_count}\")  # Print the number of new people added in this degree\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 825
        },
        "id": "Iw78mOxWn0W-",
        "outputId": "02eb5b19-4830-4521-dc4a-50001e67420b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Degree 1...\n",
            "Processing person: nm0000102\n",
            "Degree 1 saved to /content/degree_1.parquet\n",
            "Number of new people in Degree 1: 661\n",
            "Starting Degree 2...\n",
            "Processing person: nm1032600\n",
            "Processing person: nm0571106\n",
            "Processing person: nm0206097\n",
            "Processing person: nm0128370\n",
            "Processing person: nm0318889\n",
            "Processing person: nm0198408\n",
            "Processing person: nm0188975\n",
            "Processing person: nm0805127\n",
            "Processing person: nm0681785\n",
            "Processing person: nm0347375\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:KeyboardInterrupt while sending command.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
            "    response = connection.send_command(command)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/py4j/clientserver.py\", line 511, in send_command\n",
            "    answer = smart_decode(self.stream.readline()[:-1])\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/socket.py\", line 718, in readinto\n",
            "    return self._sock.recv_into(b)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-ad0e85d6b5a1>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mperson\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcurrent_degree_people\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;31m# Get all people connected to the current person (who haven't been visited)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mconnected_people\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_connected_people\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mperson\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mnew_person\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconnected_people\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-36-9c48c38bbc12>\u001b[0m in \u001b[0;36mfind_connected_people\u001b[0;34m(person_nconst)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# Collect distinct connected people\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mconnected_people\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'nconst'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mrdd\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    212\u001b[0m         \"\"\"\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lazy_rdd\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m             \u001b[0mjrdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjavaToPython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m             self._lazy_rdd = RDD(\n\u001b[1;32m    216\u001b[0m                 \u001b[0mjrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCPickleSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1322\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1038\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1039\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1040\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/clientserver.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m                 \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    512\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m                 \u001b[0;31m# Happens when a the other end is dead. There might be an empty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    716\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 718\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    719\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}